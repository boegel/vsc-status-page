<html>
	<head>
	  <meta charset="utf-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1">
	  <meta property="title" content="VSC status page - Vlaams Supercomputer Centrum" />
	  <meta property="description" content="The Flemish Supercomputer Centre (VSC) is a virtual supercomputer center for academics and industry. It is managed by the FWO with the five Flemish university associations." />
	  <meta property="keywords" content="VSC; Vlaams Supercomputer Centrum; Flemish Supercomputer Centre; supercomputing; HPC" />
	  
	  <title>VSC Tier1 Compute status page</title>

	  <link rel="icon" type="image/png" href="images/favicon.png" />
	  <link rel="stylesheet" href="style.css">
	  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto+Condensed:700,300|Open+Sans:400i,600i,700i,300,400,700,600&display=swap" crossorigin="anonymous">
	</head>
    <body>
		<div class="logo">
			<a href="https://vscentrum.be/"><img class="logo" src="images/vsc_logo.png" alt="VSC logo"/></a>
		</div>
        <h1 class="page_title">VSC Tier1 Compute status page</h1>
        
        <h2>Current incidents</h2>
        
        <h3><a id="incident_0080">Hardware defect in Tier1 Compute infrastructure</a></h3>

<table>
    <tr>
        <td><span class="td_name">Start:</span></td>
        <td>2025-02-25 08:00:00</td>
    </tr>
    <tr>
        <td><span class="td_name">End:</span></td>
        <td>Unknown</td>
    </tr>
    <tr>
        <td><span class="td_name">Updated:</span></td>
        <td>2025-04-12 21:15:14</td>
    </tr>
    <tr>
        <td><span class="td_name">Level:</span></td>
        <td>high</td>
    </tr>
</table>

<p> Considerable hardware is currently broken in the Tier1 Compute infrastructure Hortense. Site research has shown this is due to corrosion and leakage in the cooling system directly going to the processing units. Vendors ATOS (now Eviden) and APAC are aware of this issue and confirm this is due to a design issue with the cooling system. </p>
<p> The damage is extensive. A.o. 90% of all GPU nodes of the gpu_rome_a100_40 partition are unavailable. </p>
<p> We are in communication with vendors Eviden (previously ATOS) and APAC to have this damage fixed and hardware replaced as soon as possible. It is already clear that a complete retrofit of the cooling system will need to happen. This, no doubt, will require another substantial maintenance window. </p>
<p> Update 10/03/2025: an intermediate repair will already be executed starting 17/03/2025, and will likely take a couple of days. A leak in the cooling system will be fixed. No full downtime is needed this time, only some hardware racks are affected. There will be a reduction in the available hardware during these repair works, but considerable CPU and GPU nodes will remain available. </p>
<p> Update 4/04/2025: more extensive retrofit repair works to replace the direct cooling loops are scheduled for 5-16 May 2025. </p>

        
        <h2>Planned maintenance</h2>
        
        <h3><a id="incident_0037">Extensive maintenance to Tier1 infrastructure</a></h3>

<table>
    <tr>
        <td><span class="td_name">Start:</span></td>
        <td>2025-05-05 08:00:00</td>
    </tr>
    <tr>
        <td><span class="td_name">End:</span></td>
        <td>2025-05-16 18:00:00</td>
    </tr>
    <tr>
        <td><span class="td_name">Updated:</span></td>
        <td>2025-04-12 21:15:14</td>
    </tr>
    <tr>
        <td><span class="td_name">Level:</span></td>
        <td>high</td>
    </tr>
</table>

<p> Vendors ATOS (now Eviden) and APAC will replace the direct cooling systems going to all processing units. This is to counteract a design issue with the cooling system, which has led a.o. to unavailabilities and broken hardware. If all goes well, broken hardware (including a large portion of the GPU nodes in the gpu_rome_a100_40 partition) will also be replaced. </p>
<p> <strong>A complete downtime is scheduled starting Monday 5 May (08:00 CET) till Friday 16 May 2025.</strong> Tier1 Compute Hortense will be unavailable during this period. This includes workernodes as well as login and debug nodes, and the dedicated Tier1 scratch filesystem. </p>
<p> We understand that this downtime is long and inconvenient, especially with the academic Tier1 projects cutoff of 2 June looming. But there is no alternative. This action needs to happen in order to prevent further damage to the system and to get hardware repaired. </p>
<p> We actually hope the downtime can be contained to 2 weeks. We aim to limit the effective downtime as much as possible. Please take this downtime into account when planning your computational work. </p>
<p> We will make use of the downtime to initiate the gradual migration towards RHEL9 for the Milan partition.<br/> 128 nodes (out of 384) of the Milan partition will already be switched to RHEL9 when the system comes back up again. The size of the <code>cpu_milan_rhel9</code> partition will grow accordingly. Over the next few months, gradually more nodes will be migrated.<br/> Please make sure that your software/workflow is RHEL9 compliant (this is also a hard requirement for academic project applications at the next Tier1 cutoff). </p>

        
        <hr/>
        <p class="navigation">
            <a href="index.html">Overview</a> |
            <a href="tier1_compute.html">Tier-1 Compute</a> |
            <a href="tier1_cloud.html">Tier-1 Cloud</a> |
            <a href="tier1_data.html">Tier-1 Data</a> |
            <a href="backbone.html">VSC backbone</a>
        </p>
        <p class="navigation">
            <a href="tier2_antwerp.html">Tier-2 Antwerpen</a> |
            <a href="tier2_brussels.html">Tier-2 Brussel</a> |
            <a href="tier2_gent.html">Tier-2 Gent</a> |
            <a href="tier2_leuven.html">Tier-2 Leuven</a>
        </p>
        <hr/>
        
    </body>
</html>

